# Read the dataset available in /Users/chandarl/Downloads/server/comp_pd/KEO2.TBL
# The top few rows of this file are as below:
# TABLE NO.  1      DATA RECS.     1 THROUGH   900
#           ID        TIME          EF         AGE          A1          A2          A3         AL1         AL2         AL3         KEO          EO          EM        EC50           N           E          EF        PRED        RES         WRES
#   1.0000E+00  0.0000E+00  1.8850E+01  3.0580E+01  9.7699E-02  6.4910E-03  1.3000E-04  4.3415E-01  5.7008E-02  9.5510E-03  5.6119E-01  2.0508E+01  4.7678E+00  9.0496E+00  2.5286E+00  2.0508E+01  1.8850E+01  1.9975E+01 -1.1253E+00 -7.3137E-01
#   1.0000E+00  5.0000E-01  1.9910E+01  3.0580E+01  9.7699E-02  6.4910E-03  1.3000E-04  4.3415E-01  5.7008E-02  9.5510E-03  5.6119E-01  2.0508E+01  4.7678E+00  9.0496E+00  2.5286E+00  2.0500E+01  1.9910E+01  1.9971E+01 -6.1008E-02 -1.5100E-01
#   1.0000E+00  1.0000E+00  1.9510E+01  3.0580E+01  9.7699E-02  6.4910E-03  1.3000E-04  4.3415E-01  5.7008E-02  9.5510E-03  5.6119E-01  2.0508E+01  4.7678E+00  9.0496E+00  2.5286E+00  2.0332E+01  1.9510E+01  1.9893E+01 -3.8252E-01 -2.5709E-01
#   1.0000E+00  1.5000E+00  1.8670E+01  3.0580E+01  9.7699E-02  6.4910E-03  1.3000E-04  4.3415E-01  5.7008E-02  9.5510E-03  5.6119E-01  2.0508E+01  4.7678E+00  9.0496E+00  2.5286E+00  1.9620E+01  1.8670E+01  1.9585E+01 -9.1452E-01 -3.2364E-01
#   1.0000E+00  2.0000E+00  1.9030E+01  3.0580E+01  9.7699E-02  6.4910E-03  1.3000E-04  4.3415E-01  5.7008E-02  9.5510E-03  5.6119E-01  2.0508E+01  4.7678E+00  9.0496E+00  2.5286E+00  1.8150E+01  1.9030E+01  1.8949E+01  8.0875E-02  5.9570E-01
#   1.0000E+00  2.5000E+00  1.6040E+01  3.0580E+01  9.7699E-02  6.4910E-03  1.3000E-04  4.3415E-01  5.7008E-02  9.5510E-03  5.6119E-01  2.0508E+01  4.7678E+00  9.0496E+00  2.5286E+00  1.6185E+01  1.6040E+01  1.8031E+01 -1.9914E+00 -7.8885E-02
#   1.0000E+00  3.0000E+00  1.5830E+01  3.0580E+01  9.7699E-02  6.4910E-03  1.3000E-04  4.3415E-01  5.7008E-02  9.5510E-03  5.6119E-01  2.0508E+01  4.7678E+00  9.0496E+00  2.5286E+00  1.4198E+01  1.5830E+01  1.6973E+01 -1.1426E+00  7.7939E-01
#   1.0000E+00  3.5000E+00  1.1100E+01  3.0580E+01  9.7699E-02  6.4910E-03  1.3000E-04  4.3415E-01  5.7008E-02  9.5510E-03  5.6119E-01  2.0508E+01  4.7678E+00  9.0496E+00  2.5286E+00  1.2497E+01  1.1100E+01  1.5915E+01 -4.8152E+00 -9.1427E-01
#   1.0000E+00  4.0000E+00  1.0800E+01  3.0580E+01  9.7699E-02  6.4910E-03  1.3000E-04  4.3415E-01  5.7008E-02  9.5510E-03  5.6119E-01  2.0508E+01  4.7678E+00  9.0496E+00  2.5286E+00  1.1161E+01  1.0800E+01  1.4950E+01 -4.1504E+00 -3.7119E-01
#   1.0000E+00  4.5000E+00  9.9100E+00  3.0580E+01  9.7699E-02  6.4910E-03  1.3000E-04  4.3415E-01  5.7008E-02  9.5510E-03  5.6119E-01  2.0508E+01  4.7678E+00  9.0496E+00  2.5286E+00  1.0149E+01  9.9100E+00  1.4116E+01 -4.2062E+00 -3.0265E-01
#   1.0000E+00  5.0000E+00  8.7200E+00  3.0580E+01  9.7699E-02  6.4910E-03  1.3000E-04  4.3415E-01  5.7008E-02  9.5510E-03  5.6119E-01  2.0508E+01  4.7678E+00  9.0496E+00  2.5286E+00  9.3900E+00  8.7200E+00  1.3416E+01 -4.6960E+00 -5.2554E-01
# Ignore the first row and read the rest of the rows into a pandas dataframe with the column names as specified in the second row.
# The goal is to use a GAN to generate the data that is similar to the original data read from the file.
# The first column is an ID column that refers to an individual and we have many rows for each individual in this dataset.
# The generated data should be for several new individuals and so will have several different IDs not present in the original dataset. 
# But for each new individual, there would be several rows of data generated by the generator of the GAN such that the discriminator is not able to distinguish between 
# the original data and the generated data leaving aside the ID column.  

# For this first we need to create a map from the ID column to a list of rows of data for that individual consisting of all other columns except the ID column.
# The generator of the GAN should generate a list of rows of data for a new individual given a random noise vector.
# The discriminator will alternatively be fed with a list of rows of data from a real individual coming from the real dataset and a list of rows of data generated by the generator.
# The goal of the discriminator will be to distinguish between the real and the generated data, while the goal of the generator will be to fool the discriminator.
# The generator and the discriminator will be trained together using a loss function that is a combination of the loss functions of the generator and the discriminator as usual in GANs.

# The trained generator and discriminator should finally be saved to a file so that the generator can be used to generate new data for new individuals.

# Some more constraints to be adhered to by the generated data based on analyzing the original data:
#  The following columns should have the same value for all rows for a given individual: AGE, A1, A2, A3, AL1, AL2, AL3, KEO, EO, EM, EC50, N.
"""
Train a WGAN-GP on per-individual sequences from the KEO2 dataset.
MPS-safe version: avoids in-place ops that break autograd.
"""

import os
import re
from dataclasses import dataclass
from functools import partial
from typing import Dict, List

import pandas as pd
import torch
from torch import nn, autograd
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence

import pickle
from pathlib import Path

# ---------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------
root_dir = "/Users/chandarl/Documents/GitHub/drug_gan"
DATA_SOURCE = f"{root_dir}/data/KEO2.TBL"
MODEL_SAVE_PATH = f"{root_dir}/models/trained_wgan_gp.pt"

# Architecture parameters - updated for better distribution matching
BATCH_SIZE = 32  # Increased from 16 - better gradient estimates for distribution matching
NOISE_DIM = 384  # Increased from 256 - more input diversity to capture full ranges
GEN_HIDDEN_DIM = 768  # Increased from 512 - more capacity to capture diversity and edge cases
DISC_HIDDEN_DIM = 128
EPOCHS = 800  # Training from scratch with new hyperparameters  

# Training hyperparameters - optimized for better distribution matching
GEN_LR = 5e-5  # Increased from 3e-5 - generator needs more aggressive learning to escape mode collapse
DISC_LR = 1e-5  # Increased from 3e-6 - discriminator needs higher LR to learn effectively
LAMBDA_GP = 10.0  # Gradient penalty weight (keep stable)
LAMBDA_FM = 5.0  # Decreased from 15.0 - feature matching pulls to mean, causing range compression (CRITICAL)
LAMBDA_DIV = 3.0  # Increased from 0.2 - distribution matching needs boost to match full ranges (CRITICAL)
LAMBDA_TEMPORAL = 0.5  # Decreased from 1.0 - temporal loss may constrain diversity
LAMBDA_ZERO_SEQ = 1.0  # Penalty weight for all-zero sequences in fake data
TEMPORAL_SLOPE_WEIGHT = 2.0  # Weight for penalizing large slopes (first derivative)
TEMPORAL_SLOPE_CHANGE_WEIGHT = 2.0  # Weight for penalizing large slope changes (second derivative)
N_CRITIC = 5  # Increased from 1 - give discriminator more training to learn effectively
N_GEN = 5  # Increased from 3 - give generator more updates to catch up and learn full distribution
GEN_GRAD_CLIP = 2.0  # Increased from 1.0 - allow larger gradients for generator to escape mode collapse
GAP_THRESHOLD = 0.4  # Adjusted - gap increasing to 0.5-0.6, catch it earlier
DISC_WARMUP_EPOCHS = 10  # Number of epochs to train only discriminator before starting generator

torch.autograd.set_detect_anomaly(True)
DEVICE = torch.device(
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using device: {DEVICE}")

# ---------------------------------------------------------------------
# Data loading and normalization
# ---------------------------------------------------------------------
def load_dataset(file_path: str) -> pd.DataFrame:
    """Load the dataset from the file path.

    Args:
        file_path (str): The path to the dataset file.

    Returns:
        pd.DataFrame: The dataset as a pandas dataframe.
    """
    with open(file_path, "r", encoding="utf-8") as handle:
        lines = [line.rstrip() for line in handle if line.strip()]

    header_line = lines[1]
    column_names = re.split(r"\s+", header_line.strip())

    seen, unique_column_names, unique_indices = {}, [], []
    for idx, col_name in enumerate(column_names):
        if col_name not in seen:
            seen[col_name] = True
            unique_column_names.append(col_name)
            unique_indices.append(idx)

    dataframe = pd.DataFrame(columns=unique_column_names)
    for raw_line in lines[2:]:
        values = re.split(r"\s+", raw_line.strip())
        if len(values) < len(column_names):
            continue
        unique_values = [values[i] for i in unique_indices]
        row_df = pd.DataFrame([unique_values], columns=unique_column_names)
        dataframe = pd.concat([dataframe, row_df], ignore_index=True)

    dataframe = dataframe.apply(pd.to_numeric, errors="coerce")
    dataframe["ID"] = dataframe["ID"].round().astype(int)
    return dataframe


def normalize_dataframe(df: pd.DataFrame):
    """Normalize the dataframe.

    Args:
        df (pd.DataFrame): The dataframe to normalize.

    Returns:
        pd.DataFrame: The normalized dataframe.
    """
    feature_cols = [c for c in df.columns if c != "ID"]
    stats = {}
    for c in feature_cols:
        mean, std = df[c].mean(), df[c].std()
        df[c] = (df[c] - mean) / (std + 1e-8)
        stats[c] = (mean, std)
    return df, stats


def denormalize(df: pd.DataFrame, stats):
    """Denormalize the dataframe.

    Args:
        df (pd.DataFrame): The dataframe to denormalize.
        stats (dict): The statistics of the dataframe.

    Returns:
        pd.DataFrame: The denormalized dataframe.
    """
    for c, (mean, std) in stats.items():
        df[c] = df[c] * std + mean
    return df


def build_individual_map(df: pd.DataFrame) -> Dict[int, pd.DataFrame]:
    """Build the individual map from the dataframe.

    Args:
        df (pd.DataFrame): The dataframe to build the individual map from.

    Returns:
        Dict[int, pd.DataFrame]: The individual map.
    """
    grouped = {}
    for id_, g in df.groupby("ID"):
        grouped[id_] = g.drop(columns=["ID"]).reset_index(drop=True)
    return grouped


# ---------------------------------------------------------------------
# Dataset
# ---------------------------------------------------------------------
class IndividualSequenceDataset(Dataset):
    def __init__(self, individual_map: Dict[int, pd.DataFrame]):
        """Initialize the individual sequence dataset.

        Args:
            individual_map (Dict[int, pd.DataFrame]): The individual map.
        """
        self.ids = sorted(individual_map.keys())
        self.feature_columns = list(individual_map[self.ids[0]].columns)
        self.sequences = [
            torch.tensor(individual_map[i].values, dtype=torch.float32)
            for i in self.ids
        ]
        self.feature_dim = self.sequences[0].shape[1]
        self.max_length = max(s.shape[0] for s in self.sequences)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return self.sequences[idx]


def collate_individual_batches(batch: List[torch.Tensor], max_length: int):
    """Collate the individual batches.

    Args:
        batch (List[torch.Tensor]): The batch of sequences.
        max_length (int): The maximum length of the sequences.

    Returns:
        Tuple[torch.Tensor, torch.Tensor]: The padded sequences and the lengths.
    """
    lengths = torch.tensor([b.shape[0] for b in batch], dtype=torch.long)
    padded = pad_sequence(batch, batch_first=True, padding_value=0.0)
    if padded.size(1) < max_length:
        pad_extra = torch.zeros(
            padded.size(0),
            max_length - padded.size(1),
            padded.size(2)
        )
        padded = torch.cat([padded, pad_extra], dim=1)
    return padded, lengths


# ---------------------------------------------------------------------
# Models
# ---------------------------------------------------------------------
class Generator(nn.Module):
    def __init__(self, noise_dim, hidden_dim, seq_len, feat_dim, feat_cols):
        """Initialize the generator.

        Args:
            noise_dim (int): The dimension of the noise vector.
            hidden_dim (int): The dimension of the hidden layers.
            seq_len (int): The length of the sequences.
            feat_dim (int): The dimension of the features.
            feat_cols (List[str]): The columns of the features.
        """
        super().__init__()
        self.sequence_length = seq_len
        self.feature_dim = feat_dim
        self.feature_columns = feat_cols

        const_cols = [
            "AGE",
            "A1",
            "A2",
            "A3",
            "AL1",
            "AL2",
            "AL3",
            "KEO",
            "EO",
            "EM",
            "EC50",
            "N",
        ]
        self.constant_indices = [
            feat_cols.index(c) for c in const_cols if c in feat_cols
        ]

        # Improved generator with more capacity and better architecture
        # Increased depth and width to capture more diversity
        self.net = nn.Sequential(
            nn.Linear(noise_dim, hidden_dim * 4),
            nn.BatchNorm1d(hidden_dim * 4),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Dropout(0.1),  # Small dropout to prevent overfitting to specific modes
            nn.Linear(hidden_dim * 4, hidden_dim * 3),
            nn.BatchNorm1d(hidden_dim * 3),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim * 3, hidden_dim * 2),
            nn.BatchNorm1d(hidden_dim * 2),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Linear(hidden_dim, seq_len * feat_dim),
            # Removed Tanh to allow wider output range - normalization handles scaling
        )

        # Better initialization for diversity
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # Use orthogonal initialization for better gradient flow
                nn.init.orthogonal_(m.weight, gain=1.0)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)

    def forward(self, z):
        """Forward pass of the generator.

        Args:
            z (torch.Tensor): The noise vector.

        Returns:
            torch.Tensor: The generated sequences.
        """
        b = z.size(0)
        out = self.net(z).view(b, self.sequence_length, self.feature_dim).clone()

        # Constant columns replicated
        out_const = out.clone()
        for idx in self.constant_indices:
            # Use first value instead of mean to preserve more diversity
            val = out_const[:, 0:1, idx].expand(-1, self.sequence_length)
            out_const = out_const.clone()
            out_const[:, :, idx] = val

        return out_const


class Discriminator(nn.Module):
    def __init__(self, seq_len, feat_dim, hidden_dim):
        """Initialize the discriminator.

        Args:
            seq_len (int): The length of the sequences.
            feat_dim (int): The dimension of the features.
            hidden_dim (int): The dimension of the hidden layers.
        """
        super().__init__()
        input_dim = seq_len * feat_dim
        self.main = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Linear(hidden_dim // 2, 1),
        )

    def forward(self, x):
        """Forward pass of the discriminator.

        Args:
            x (torch.Tensor): The input sequences.

        Returns:
            torch.Tensor: The output of the discriminator.
        """
        return self.main(x.view(x.size(0), -1))


# ---------------------------------------------------------------------
# Diversity and Feature Matching Losses
# ---------------------------------------------------------------------
def is_valid_sequence(sequences):
    """
    Detect valid (non-padded) time steps within sequences.
    For each time step in each sequence, check if all features are zero.
    
    Args:
        sequences: (batch_size, seq_len, feature_dim) tensor
                   Each element in the batch represents one individual
                   seq_len dimension contains multiple time steps for that individual
                   feature_dim contains all the features at each time step for that individual
    
    Returns:
        (batch_size, seq_len) boolean mask where True indicates valid (non-padded) time steps
    """
    # For each time step in each sequence, take absolute value of all features and sum along feature dimension
    # Sum absolute values along features (dim=2): (batch_size, seq_len)
    feature_sums = sequences.abs().sum(dim=2)  # (batch_size, seq_len)
    
    # A time step is invalid if sum of all features is zero (all features are zero)
    # A time step is valid if it has at least some non-zero feature values
    valid_mask = feature_sums > 1e-6  # (batch_size, seq_len)
    return valid_mask


def compute_zero_sequence_penalty(fake_samples):
    """
    Penalize fake time steps where all features are zero.
    For each time step, checks if all features are zero and penalizes such cases.
    
    Args:
        fake_samples: (batch_size, seq_len, feature_dim) generated sequences
    
    Returns:
        Loss value penalizing all-zero time steps (higher penalty for more all-zero time steps)
    """
    # For each time step, sum absolute values of all features: (batch_size, seq_len)
    feature_sums = fake_samples.abs().sum(dim=2)  # (batch_size, seq_len)
    
    # Penalize time steps where all features are zero (sum is zero or very close to zero)
    # Use exponential penalty that increases as time steps approach zero
    # This encourages the generator to produce non-zero features at each time step
    zero_penalty = torch.mean(torch.exp(-feature_sums / 1e-3))
    
    return zero_penalty


def compute_diversity_loss(real_samples, fake_samples):
    """Compute distribution matching loss using Wasserstein distance.
    Constructs probability distributions per feature for valid real samples and all fake samples,
    then computes Wasserstein-1 distance between them.
    
    Args:
        real_samples (torch.Tensor): Real samples (batch_size, seq_len, feature_dim)
        fake_samples (torch.Tensor): Generated samples (batch_size, seq_len, feature_dim)
    
    Returns:
        torch.Tensor: Wasserstein distance penalty (lower is better - fake matches real distribution)
    """
    batch_size, seq_len, feature_dim = real_samples.shape
    device = real_samples.device
    
    # Get mask for valid real samples: (batch_size, seq_len)
    real_valid_mask = is_valid_sequence(real_samples)  # (batch_size, seq_len)
    
    # Check if we have any valid real samples
    if not real_valid_mask.any():
        return torch.tensor(0.0, device=device)
    
    # For each feature, construct probability distributions
    wasserstein_distances = []
    
    for feat_idx in range(feature_dim):
        # Extract feature values from valid real samples only
        # Get feature values: (batch_size, seq_len)
        real_feat_2d = real_samples[:, :, feat_idx]  # (batch_size, seq_len)
        # Use boolean mask to extract only valid values
        real_feat_values = real_feat_2d[real_valid_mask]  # (n_valid_real,)
        
        # Extract feature values from all fake samples (no filtering)
        fake_feat_values = fake_samples[:, :, feat_idx].flatten()  # (batch_size * seq_len,)
        
        # Skip if no valid real samples for this feature
        if real_feat_values.shape[0] == 0:
            continue
        
        # Ensure we have samples from both distributions
        if fake_feat_values.shape[0] == 0:
            continue
        
        # Use the minimum number of samples for fair comparison
        min_samples = min(real_feat_values.shape[0], fake_feat_values.shape[0])
        if min_samples == 0:
            continue
        
        # Sample or use all available samples (use minimum for fair comparison)
        real_feat = real_feat_values[:min_samples]  # (min_samples,)
        fake_feat = fake_feat_values[:min_samples]  # (min_samples,)
        
        # Sort samples to compute Wasserstein-1 distance
        # Wasserstein-1 distance between two 1D distributions is the L1 distance between sorted samples
        real_sorted, _ = torch.sort(real_feat)
        fake_sorted, _ = torch.sort(fake_feat)
        
        # Wasserstein-1 distance: mean absolute difference between sorted samples
        wasserstein_1d = torch.mean(torch.abs(real_sorted - fake_sorted))
        wasserstein_distances.append(wasserstein_1d)
    
    # Average Wasserstein distance across all feature dimensions
    if len(wasserstein_distances) == 0:
        return torch.tensor(0.0, device=device)
    
    mean_wasserstein = torch.stack(wasserstein_distances).mean()
    return mean_wasserstein


def compute_feature_matching_loss(real_samples, fake_samples):
    """Feature matching loss: encourages generated samples to match real data statistics.
    Computes L2 distance between mean and std of real vs fake samples.

    Args:
        real_samples (torch.Tensor): The real samples (batch_size, seq_len, feature_dim).
        fake_samples (torch.Tensor): The generated samples (batch_size, seq_len, feature_dim).

    Returns:
        torch.Tensor: The feature matching loss.
    """
    # Filter out zero-padded time steps in real samples only
    # Get mask for each time step: (batch_size, seq_len)
    real_valid_mask = is_valid_sequence(real_samples)  # (batch_size, seq_len)
    
    # Check if we have any valid time steps
    if not real_valid_mask.any():
        return torch.tensor(0.0, device=real_samples.device)
    
    batch_size, seq_len, feature_dim = real_samples.shape
    
    # Compute statistics per feature, using only valid time steps for real samples
    real_means = []
    real_stds = []
    fake_means = []
    fake_stds = []
    
    for feat_idx in range(feature_dim):
        # Extract feature values from valid real time steps only
        real_feat_2d = real_samples[:, :, feat_idx]  # (batch_size, seq_len)
        real_feat_valid = real_feat_2d[real_valid_mask]  # (n_valid_time_steps,)
        
        # Extract feature values from all fake time steps (no filtering)
        fake_feat_all = fake_samples[:, :, feat_idx].flatten()  # (batch_size * seq_len,)
        
        # Compute statistics
        if real_feat_valid.shape[0] > 0:
            real_means.append(real_feat_valid.mean())
            real_stds.append(real_feat_valid.std())
        else:
            real_means.append(torch.tensor(0.0, device=real_samples.device))
            real_stds.append(torch.tensor(0.0, device=real_samples.device))
        
        fake_means.append(fake_feat_all.mean())
        fake_stds.append(fake_feat_all.std())
    
    # Stack into tensors
    real_mean = torch.stack(real_means)  # (feature_dim,)
    real_std = torch.stack(real_stds)    # (feature_dim,)
    fake_mean = torch.stack(fake_means)  # (feature_dim,)
    fake_std = torch.stack(fake_stds)   # (feature_dim,)
    
    # L2 loss between statistics
    mean_loss = torch.nn.functional.mse_loss(fake_mean, real_mean)
    std_loss = torch.nn.functional.mse_loss(fake_std, real_std)
    
    return mean_loss + std_loss


def compute_temporal_pattern_loss(fake_samples, feature_cols, slope_weight=1.0, slope_change_weight=1.0):
    """
    Temporal pattern loss: ensures generated sequences have smoothly changing features over time.
    Uses linear interpolation and penalizes both large slopes and large slope changes.
    
    Args:
        fake_samples: (batch_size, seq_len, feature_dim) generated sequences
        feature_cols: List of feature column names
        slope_weight: Weight for penalizing large slopes (first derivative)
        slope_change_weight: Weight for penalizing large slope changes (second derivative)
    
    Returns:
        Loss value penalizing non-smooth temporal patterns in fake data
    """
    # Find indices for TIME, EF, E, PRED
    try:
        time_idx = feature_cols.index('TIME')
        ef_idx = feature_cols.index('EF')
        e_idx = feature_cols.index('E')
        pred_idx = feature_cols.index('PRED')
    except ValueError:
        # If columns don't exist, return zero loss
        return torch.tensor(0.0, device=fake_samples.device)
    
    batch_size, seq_len, _ = fake_samples.shape
    device = fake_samples.device
    total_loss = torch.tensor(0.0, device=device)
    
    # Number of evaluation points for interpolated functions
    N_EVAL_POINTS = 50
    
    # Process each sequence in the batch
    for batch_idx in range(batch_size):
        
        # Extract TIME and feature values for this sample
        time_sample = fake_samples[batch_idx, :, time_idx]  # (seq_len,)
        
        # Skip if not enough points for interpolation (need at least 2)
        if seq_len < 2:
            continue
        
        # Sort by time to ensure temporal order
        sort_idx = torch.argsort(time_sample)
        time_sorted = time_sample[sort_idx]
        
        # Get time range for interpolation
        t_min, t_max = time_sorted[0], time_sorted[-1]
        
        # Skip if time range is too small
        if t_max - t_min < 1e-6:
            continue
        
        # Create evaluation time points (uniformly spaced)
        eval_times = torch.linspace(t_min, t_max, N_EVAL_POINTS, device=device)
        
        # For each feature (EF, E, PRED), compute smoothness loss
        for feat_idx in [ef_idx, e_idx, pred_idx]:
            feat_sample = fake_samples[batch_idx, :, feat_idx]  # (seq_len,)
            feat_sorted = feat_sample[sort_idx]
            
            # Linear interpolation: evaluate interpolated function
            eval_values = _linear_interpolate(time_sorted, feat_sorted, eval_times)
            
            # Compute slopes (first derivatives) of the interpolated function
            # Slope = change in feature / change in time
            dt = eval_times[1:] - eval_times[:-1]  # (N_EVAL_POINTS-1,)
            dfeat = eval_values[1:] - eval_values[:-1]  # (N_EVAL_POINTS-1,)
            slopes = dfeat / (dt + 1e-8)  # (N_EVAL_POINTS-1,)
            
            # Normalize by feature magnitude for scale-invariance
            feat_std = torch.std(feat_sorted) + 1e-8
            
            # Penalty 1: Large slopes (first derivative)
            # Penalize large absolute slopes to prevent rapid feature changes
            slope_penalty = torch.mean(slopes ** 2)
            slope_penalty_norm = slope_penalty / (feat_std ** 2 + 1e-8)
            
            # Penalty 2: Large slope changes (second derivative)
            # Penalize large changes in slopes to ensure smoothness/continuity
            if slopes.shape[0] > 1:
                slope_changes = slopes[1:] - slopes[:-1]  # (N_EVAL_POINTS-2,)
                slope_change_penalty = torch.mean(slope_changes ** 2)
            else:
                slope_change_penalty = torch.tensor(0.0, device=device)
            slope_change_penalty_norm = slope_change_penalty / (feat_std ** 2 + 1e-8)
            
            # Combined loss with weights
            feature_loss = slope_weight * slope_penalty_norm + slope_change_weight * slope_change_penalty_norm
            total_loss = total_loss + feature_loss
    
    # Normalize by number of features and batch size
    n_features = 3  # EF, E, PRED
    if batch_size > 0:
        return total_loss / (batch_size * n_features)
    else:
        return torch.tensor(0.0, device=device)


def _linear_interpolate(times, values, eval_times):
    """
    Linear interpolation: for each time in eval_times, find the interpolated value
    based on the (times, values) pairs.
    
    Args:
        times: (n_points,) sorted time points
        values: (n_points,) corresponding feature values
        eval_times: (n_eval,) time points at which to evaluate
    
    Returns:
        (n_eval,) interpolated values
    """
    n_points = times.shape[0]
    
    # For each eval_time, find the interval [times[i], times[i+1]] it falls into
    indices = torch.searchsorted(times, eval_times, right=False)  # (n_eval,)
    indices = torch.clamp(indices, 0, n_points - 2)
    
    # Get the two time points and values for interpolation
    t0 = times[indices]  # (n_eval,)
    t1 = times[indices + 1]  # (n_eval,)
    v0 = values[indices]  # (n_eval,)
    v1 = values[indices + 1]  # (n_eval,)
    
    # Linear interpolation: v = v0 + (v1 - v0) * (t - t0) / (t1 - t0)
    dt = torch.clamp(t1 - t0, min=1e-8)
    alpha = torch.clamp((eval_times - t0) / dt, 0.0, 1.0)
    
    return v0 + alpha * (v1 - v0)


# ---------------------------------------------------------------------
# Gradient Penalty
# ---------------------------------------------------------------------
def compute_gp(D, real_samples, fake_samples):
    """
    Compute the gradient penalty term for WGAN-GP (Wasserstein GAN with Gradient Penalty).
    
    Purpose:
    - In WGAN, the discriminator (critic) must be 1-Lipschitz continuous to compute the 
      Wasserstein distance correctly. This means the gradient norm ||âˆ‡D(x)|| should be â‰¤ 1 
      everywhere in the data space.
    - Instead of weight clipping (which can cause training instability), we enforce this 
      constraint by penalizing gradients that deviate from 1.
    
    How it works:
    1. Create interpolated samples between real and fake samples (random convex combinations)
    2. Compute discriminator output on these interpolated samples
    3. Compute gradients of discriminator output w.r.t. the interpolated samples
    4. Penalize gradients whose L2 norm deviates from 1 (the Lipschitz constraint)
    
    The penalty term: LAMBDA_GP * mean((||âˆ‡D(xÌ‚)||â‚‚ - 1)Â²)
    where xÌ‚ = Î± * x_real + (1 - Î±) * x_fake, Î± ~ Uniform(0,1)
    
    This encourages the discriminator to have gradients with norm â‰ˆ 1 everywhere along
    the path between real and fake samples, ensuring stable training.
    """
    # Step 1: Create random interpolation coefficients Î± ~ Uniform(0,1)
    # Each sample in the batch gets its own random Î± value
    alpha = torch.rand(real_samples.size(0), 1, 1, device=real_samples.device)
    alpha = alpha.expand_as(real_samples)  # Expand to match sample dimensions
    
    # Step 2: Create interpolated samples xÌ‚ = Î± * x_real + (1 - Î±) * x_fake
    # These are random points on the line segment between each real and fake sample pair
    # We need requires_grad=True to compute gradients w.r.t. these interpolated samples
    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(
        True
    )
    
    # Step 3: Compute discriminator output on interpolated samples
    d_interpolates = D(interpolates)
    
    # Step 4: Compute gradients of discriminator output w.r.t. interpolated samples
    # We use autograd.grad to get âˆ‡D(xÌ‚), the gradient of the discriminator at each interpolated point
    # grad_outputs=fake (all ones) means we're computing the gradient of the sum of outputs
    fake = torch.ones(d_interpolates.size(), device=real_samples.device, requires_grad=False)
    gradients = autograd.grad(
        outputs=d_interpolates,      # Discriminator outputs
        inputs=interpolates,         # W.r.t. interpolated samples
        grad_outputs=fake,           # Backprop through sum of outputs
        create_graph=True,           # Need to compute second-order gradients
        retain_graph=True,           # Keep computation graph for backprop
        only_inputs=True,            # Only return gradients w.r.t. inputs
    )[0]
    
    # Step 5: Flatten gradients to compute L2 norm per sample
    # gradients shape: (batch_size, seq_len, feature_dim) -> (batch_size, seq_len * feature_dim)
    gradients = gradients.view(gradients.size(0), -1)
    
    # Step 6: Compute penalty = LAMBDA_GP * mean((||âˆ‡D(xÌ‚)||â‚‚ - 1)Â²)
    # This penalizes gradients that are too large (>1) or too small (<1)
    # The squared term ensures we penalize deviations in both directions
    gradient_norms = gradients.norm(2, dim=1)  # L2 norm per sample: ||âˆ‡D(xÌ‚)||â‚‚
    penalty = ((gradient_norms - 1) ** 2).mean() * LAMBDA_GP
    
    return penalty


# ---------------------------------------------------------------------
# Training
# ---------------------------------------------------------------------
@dataclass
class TrainingArtifacts:
    generator: Generator
    discriminator: Discriminator
    gen_losses: List[float]
    disc_losses: List[float]


def train_wgan_gp(loader, G, D, device, noise_dim, feature_cols, start_epoch=0, previous_g_losses=None, previous_d_losses=None):
    # Better optimizer settings for generator
    opt_G = torch.optim.Adam(G.parameters(), lr=GEN_LR, betas=(0.5, 0.999))
    opt_D = torch.optim.Adam(D.parameters(), lr=DISC_LR, betas=(0.5, 0.9))
    
    # Learning rate schedulers - slower decay to maintain learning capacity
    scheduler_G = torch.optim.lr_scheduler.ExponentialLR(opt_G, gamma=0.9995)  # Very slow decay for generator
    scheduler_D = torch.optim.lr_scheduler.ExponentialLR(opt_D, gamma=0.999)  # Slower decay for discriminator (was 0.995)
    
    # Adjust schedulers to account for starting epoch
    if start_epoch > 0:
        for _ in range(start_epoch):
            scheduler_G.step()
            scheduler_D.step()
        print(f"Adjusted learning rate schedulers for epoch {start_epoch}")

    # Initialize losses with previous losses if resuming
    g_losses = previous_g_losses if previous_g_losses is not None else []
    d_losses = previous_d_losses if previous_d_losses is not None else []
    
    if start_epoch > 0:
        print(f"Resuming training from epoch {start_epoch + 1}")

    for epoch in range(start_epoch, EPOCHS):
        d_fake_val = d_real_val = 0.0  # Initialize diagnostics
        diversity_val = 0.0
        is_warmup = epoch < DISC_WARMUP_EPOCHS
        
        if is_warmup and epoch == start_epoch:
            print(f"ðŸ”¥ Discriminator warm-up phase: Training discriminator only for {DISC_WARMUP_EPOCHS} epochs")
        
        if not is_warmup and epoch == DISC_WARMUP_EPOCHS:
            print(f"âœ… Warm-up complete! Starting generator training from epoch {DISC_WARMUP_EPOCHS + 1}")
        
        for i, (real_batch, _) in enumerate(loader):
            real_batch = real_batch.to(device)
            bsz = real_batch.size(0)

            # ---------------------
            #  Train Discriminator
            # ---------------------
            for _ in range(N_CRITIC):
                z = torch.randn(bsz, noise_dim, device=device)
                fake = G(z).detach()
                loss_D = D(fake).mean() - D(real_batch).mean()
                gp = compute_gp(D, real_batch, fake)
                d_loss = loss_D + gp

                opt_D.zero_grad(set_to_none=True)
                d_loss.backward()
                # Gradient clipping for discriminator stability
                torch.nn.utils.clip_grad_norm_(D.parameters(), max_norm=1.0)
                opt_D.step()

            # -----------------
            #  Train Generator (skip during warm-up)
            # -----------------
            if not is_warmup:
                for _ in range(N_GEN):
                    z = torch.randn(bsz, noise_dim, device=device)
                    fake = G(z)
                    
                    # Adversarial loss
                    adv_loss = -D(fake).mean()
                    
                    # Feature matching loss: match real data statistics
                    fm_loss = compute_feature_matching_loss(real_batch, fake)
                    
                    # Distribution matching loss: encourage fake data to follow real data distribution (Wasserstein distance)
                    div_loss = compute_diversity_loss(real_batch, fake)
                    
                    # Temporal pattern loss: ensure smooth temporal patterns in fake data
                    temporal_loss = compute_temporal_pattern_loss(
                        fake, feature_cols, 
                        slope_weight=TEMPORAL_SLOPE_WEIGHT,
                        slope_change_weight=TEMPORAL_SLOPE_CHANGE_WEIGHT
                    )
                    
                    # Zero sequence penalty: penalize all-zero sequences (where all features are zero for an individual)
                    zero_seq_penalty = compute_zero_sequence_penalty(fake)
                    
                    # Combined generator loss
                    g_loss = adv_loss + LAMBDA_FM * fm_loss + LAMBDA_DIV * div_loss + LAMBDA_TEMPORAL * temporal_loss + LAMBDA_ZERO_SEQ * zero_seq_penalty

                    opt_G.zero_grad(set_to_none=True)
                    g_loss.backward()
                    # Gradient clipping for generator stability
                    torch.nn.utils.clip_grad_norm_(G.parameters(), max_norm=GEN_GRAD_CLIP)
                    opt_G.step()
                
                g_losses.append(g_loss.item())
            else:
                # During warm-up, append a dummy value or skip generator loss tracking
                # We'll use a placeholder value for consistency
                g_losses.append(0.0)  # Placeholder during warm-up
            
            d_losses.append(d_loss.item())
            
            # Diagnostic: compute D(fake) and D(real) for monitoring (only on last batch of epoch)
            # IMPORTANT: D(real) should be > D(fake) for a good discriminator
            # If D(real) < D(fake), the discriminator is not learning correctly
            if i == len(loader) - 1:
                with torch.no_grad():
                    z_diag = torch.randn(bsz, noise_dim, device=device)
                    fake_diag = G(z_diag)
                    d_fake_val = D(fake_diag).mean().item()
                    d_real_val = D(real_batch).mean().item()
                    
                    # Compute diversity metric for monitoring (MPS-compatible)
                    fake_flat = fake_diag.view(bsz, -1)
                    fake_expanded_i = fake_flat.unsqueeze(1)
                    fake_expanded_j = fake_flat.unsqueeze(0)
                    squared_diff = (fake_expanded_i - fake_expanded_j) ** 2
                    squared_distances = squared_diff.sum(dim=2)
                    diversity_metric = torch.sqrt(squared_distances + 1e-8)
                    mask = torch.triu(torch.ones(bsz, bsz, device=device), diagonal=1).bool()
                    diversity_val = diversity_metric[mask].mean().item() if bsz > 1 else 0.0

        # Calculate gap for monitoring (signed: positive means D(real) > D(fake), which is correct)
        gap_signed = d_real_val - d_fake_val
        gap = abs(gap_signed)
        
        # Adaptive learning rate adjustment based on gap and discriminator correctness
        current_d_lr = scheduler_D.get_last_lr()[0]
        current_g_lr = scheduler_G.get_last_lr()[0]
        
        # Track if we manually adjusted learning rates (so we can update scheduler base_lrs)
        d_lr_adjusted = False
        g_lr_adjusted = False
        new_d_lr = current_d_lr
        new_g_lr = current_g_lr
        
        # Check if discriminator is learning correctly (D(real) should be > D(fake))
        if gap_signed < 0:
            # Discriminator is backwards: D(real) < D(fake) - discriminator needs MORE training
            if current_d_lr < 2e-5:  # Only increase if not too high
                # If LR is very low (< 1e-6), reset to minimum effective LR instead of tiny increase
                if current_d_lr < 1e-6:
                    new_d_lr = 1e-5  # Reset to minimum effective LR
                else:
                    new_d_lr = min(current_d_lr * 1.5, 2e-5)  # Increase by 50%, cap at 2e-5
                for param_group in opt_D.param_groups:
                    param_group['lr'] = new_d_lr
                d_lr_adjusted = True
                print(f"  âš ï¸  Discriminator backwards (D(real)={d_real_val:.4f} < D(fake)={d_fake_val:.4f}), increasing D LR from {current_d_lr:.8f} to {new_d_lr:.8f}")
            
            # Reduce generator LR if it's too high (generator might be too strong) - skip during warm-up
            if not is_warmup and current_g_lr > 1e-5:
                new_g_lr = max(current_g_lr * 0.9, 1e-5)  # Reduce by 10%, floor at 1e-5
                for param_group in opt_G.param_groups:
                    param_group['lr'] = new_g_lr
                g_lr_adjusted = True
                print(f"  âš ï¸  Discriminator backwards, reducing G LR from {current_g_lr:.8f} to {new_g_lr:.8f}")
        
        elif gap > GAP_THRESHOLD:
            # Discriminator is correct (D(real) > D(fake)) but gap is too large - discriminator too strong
            # Reduce discriminator LR if it's not too low already
            if current_d_lr > 1e-8:
                new_d_lr = current_d_lr * 0.9  # Reduce by 10%
                for param_group in opt_D.param_groups:
                    param_group['lr'] = new_d_lr
                d_lr_adjusted = True
                print(f"  âš ï¸  Gap too large ({gap:.4f}, D(real)={d_real_val:.4f} > D(fake)={d_fake_val:.4f}), reducing D LR from {current_d_lr:.8f} to {new_d_lr:.8f}")
            
            # Increase generator LR if gap is very large and generator LR is not too high - skip during warm-up
            if not is_warmup and gap > GAP_THRESHOLD * 1.5 and current_g_lr < 5e-5:
                new_g_lr = min(current_g_lr * 1.1, 5e-5)  # Increase by 10%, cap at 5e-5
                for param_group in opt_G.param_groups:
                    param_group['lr'] = new_g_lr
                g_lr_adjusted = True
                print(f"  âš ï¸  Gap very large ({gap:.4f}), increasing G LR from {current_g_lr:.8f} to {new_g_lr:.8f}")
        
        # Update scheduler base_lrs if we manually adjusted learning rates
        # This ensures the scheduler continues from the new LR instead of overwriting it
        if d_lr_adjusted:
            # Update scheduler's base_lrs so that after step(), the LR will be new_d_lr
            # For ExponentialLR: after step(), lr = base_lr * gamma^(last_epoch + 1)
            # We want: new_d_lr = base_lr_new * gamma^(last_epoch + 1)
            # So: base_lr_new = new_d_lr / gamma^(last_epoch + 1)
            scheduler_D.base_lrs = [new_d_lr / (scheduler_D.gamma ** (scheduler_D.last_epoch + 1)) for _ in scheduler_D.base_lrs]
        
        if g_lr_adjusted:
            scheduler_G.base_lrs = [new_g_lr / (scheduler_G.gamma ** (scheduler_G.last_epoch + 1)) for _ in scheduler_G.base_lrs]
        
        # Update learning rates (schedulers) - skip discriminator decay if it's backwards
        scheduler_G.step()
        # Only decay discriminator LR if it's learning correctly (not backwards)
        # This prevents the scheduler from fighting our manual LR increases
        if gap_signed >= 0:  # Discriminator is correct or at least not backwards
            scheduler_D.step()
        # If discriminator is backwards, we skip scheduler step to preserve the increased LR
        
        # Override scheduler's LR with our manually adjusted values if we changed them
        # This ensures the manual adjustment persists even if scheduler recalculates
        if d_lr_adjusted:
            for param_group in opt_D.param_groups:
                param_group['lr'] = new_d_lr
        
        if g_lr_adjusted:
            for param_group in opt_G.param_groups:
                param_group['lr'] = new_g_lr
        
        # Compute loss components for logging (only on last batch)
        if not is_warmup:
            with torch.no_grad():
                z_log = torch.randn(bsz, noise_dim, device=device)
                fake_log = G(z_log)
                adv_loss_log = -D(fake_log).mean().item()
                fm_loss_log = compute_feature_matching_loss(real_batch, fake_log).item()
                div_loss_log = compute_diversity_loss(real_batch, fake_log).item()
                temporal_loss_log = compute_temporal_pattern_loss(
                    fake_log, feature_cols,
                    slope_weight=TEMPORAL_SLOPE_WEIGHT,
                    slope_change_weight=TEMPORAL_SLOPE_CHANGE_WEIGHT
                ).item()
            g_loss_log = g_loss.item()
        else:
            # During warm-up, use placeholder values
            adv_loss_log = fm_loss_log = div_loss_log = temporal_loss_log = 0.0
            g_loss_log = 0.0
        
        warmup_indicator = " [WARMUP]" if is_warmup else ""
        print(f"Epoch {epoch+1}/{EPOCHS}{warmup_indicator} | D: {d_loss.item():.4f} | G: {g_loss_log:.4f} (adv:{adv_loss_log:.4f} fm:{fm_loss_log:.4f} div:{div_loss_log:.4f} temp:{temporal_loss_log:.4f}) | D(real): {d_real_val:.4f} | D(fake): {d_fake_val:.4f} | Gap: {gap_signed:+.4f} | Diversity: {diversity_val:.4f} | LR_G: {scheduler_G.get_last_lr()[0]:.6f} | LR_D: {opt_D.param_groups[0]['lr']:.8f}")

    # Final epoch is the last completed epoch (epoch is 0-indexed, so final epoch is epoch + 1)
    # If we completed all epochs, epoch would be EPOCHS - 1, so final_epoch = EPOCHS
    final_epoch = epoch + 1
    return TrainingArtifacts(G, D, g_losses, d_losses), final_epoch


# ---------------------------------------------------------------------
# Save / Load
# ---------------------------------------------------------------------
def save_models_with_stats(artifacts, path, stats, epoch=None):
    """Save models + normalization stats together."""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    save_dict = {
        "G": artifacts.generator.state_dict(),
        "D": artifacts.discriminator.state_dict(),
        "g_losses": artifacts.gen_losses,
        "d_losses": artifacts.disc_losses,
    }
    if epoch is not None:
        save_dict["epoch"] = epoch
    torch.save(save_dict, path)
    print(f"Saved model weights to {path}")

    stats_path = Path(path).with_suffix(".stats.pkl")
    with open(stats_path, "wb") as f:
        pickle.dump(stats, f)
    print(f"Saved normalization stats to {stats_path}")


def load_models(path, G, D, device):
    """Load saved model weights and return checkpoint, previous losses, and epoch."""
    if not os.path.exists(path):
        return None, [], [], 0
    
    print(f"Loading saved model from {path}")
    checkpoint = torch.load(path, map_location=device)
    
    G.load_state_dict(checkpoint["G"])
    D.load_state_dict(checkpoint["D"])
    
    g_losses = checkpoint.get("g_losses", [])
    d_losses = checkpoint.get("d_losses", [])
    saved_epoch = checkpoint.get("epoch", None)
    
    print("Loaded model weights successfully")
    print(f"Previous losses: G={len(g_losses)} entries, D={len(d_losses)} entries")
    if saved_epoch is not None:
        print(f"Saved epoch: {saved_epoch}")
    
    return checkpoint, g_losses, d_losses, saved_epoch


# ---------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------
def main():
    torch.manual_seed(0)
    df = load_dataset(DATA_SOURCE)
    df, stats = normalize_dataframe(df)
    indiv_map = build_individual_map(df)
    dataset = IndividualSequenceDataset(indiv_map)
    loader = DataLoader(
        dataset,
        batch_size=min(BATCH_SIZE, len(dataset)),
        shuffle=True,
        collate_fn=partial(collate_individual_batches, max_length=dataset.max_length),
    )

    G = Generator(NOISE_DIM, GEN_HIDDEN_DIM, dataset.max_length,
                  dataset.feature_dim, dataset.feature_columns).to(DEVICE)
    D = Discriminator(dataset.max_length, dataset.feature_dim, DISC_HIDDEN_DIM).to(DEVICE)

    # Try to load existing model
    checkpoint, previous_g_losses, previous_d_losses, saved_epoch = load_models(
        MODEL_SAVE_PATH, G, D, DEVICE
    )
    
    # Determine starting epoch
    start_epoch = 0
    if checkpoint is not None:
        if saved_epoch is not None:
            # Use saved epoch if available (most accurate)
            start_epoch = saved_epoch
            print(f"Resuming from saved epoch: {start_epoch}")
        elif previous_g_losses:
            # Estimate epochs from number of loss entries (losses are saved per batch)
            batches_per_epoch = len(loader)
            estimated_epochs = len(previous_g_losses) // batches_per_epoch
            start_epoch = estimated_epochs
            print(f"Estimated starting epoch: {start_epoch} (from {len(previous_g_losses)} loss entries, {batches_per_epoch} batches/epoch)")
    
    artifacts, final_epoch = train_wgan_gp(loader, G, D, DEVICE, NOISE_DIM, dataset.feature_columns,
                              start_epoch=start_epoch,
                              previous_g_losses=previous_g_losses,
                              previous_d_losses=previous_d_losses)
    save_models_with_stats(artifacts, MODEL_SAVE_PATH, stats, epoch=final_epoch)


if __name__ == "__main__":
    main()
